# Skip2-LoRA vs Traditional LoRA è©•ä¾¡ã‚¬ã‚¤ãƒ‰

## æ¦‚è¦

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€Skip2-LoRA ã¨å¾“æ¥ã®LoRA ã®æ€§èƒ½ã‚’æ¯”è¼ƒã™ã‚‹ãŸã‚ã®å®Ÿé¨“æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## 1. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œï¼ˆæ¨å¥¨ï¼šæœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ï¼‰

ã¾ãšã€è¨ˆç®—é€Ÿåº¦ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã—ã¾ã™ã€‚

```bash
python benchmark_skip2lora.py
```

### å‡ºåŠ›ä¾‹

```
==============================================================================
Skip2-LoRA Performance Evaluation
==============================================================================

Evaluating: Traditional LoRA (Query+Value)
======================================================================
Creating model...
Counting parameters...
  Total Parameters: XXX,XXX,XXX
  Trainable Parameters: X,XXX,XXX
  LoRA Parameters: X,XXX,XXX
  Trainable Ratio: XX.XX%

Benchmarking forward pass...
  Mean: XXX.XXms
  ...

Benchmarking backward pass...
  Mean: XXX.XXms
  ...

COMPARISON SUMMARY
======================================================================
ğŸ“Š Parameter Count Comparison
...
âš¡ Forward Pass Benchmark
...
âš™ï¸ Backward Pass Benchmark
...
ğŸ’¾ Memory Usage
...
```

## 2. å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã®æ¯”è¼ƒ

### ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆå¾“æ¥LoRAï¼‰

```bash
litgpt finetune config_hub/finetune/llama-2-7b/lora_baseline.yaml \
  --out_dir out/finetune/lora_baseline_exp
```

ã“ã®å®Ÿé¨“ã§ã¯ä»¥ä¸‹ã‚’æ¸¬å®šã—ã¾ã™ï¼š
- **å­¦ç¿’æ™‚é–“** (step / sec)
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡** (ãƒ”ãƒ¼ã‚¯GPUãƒ¡ãƒ¢ãƒª)
- **æå¤±å€¤ã®æ¨ç§»** (validation loss)

### ã‚¹ãƒ†ãƒƒãƒ—2: Skip2-LoRA (4å±¤)ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

```bash
litgpt finetune config_hub/finetune/llama-2-7b/skip2lora.yaml \
  --out_dir out/finetune/skip2lora_4blocks_exp \
  --skip2lora_block_indices '[0, 1, 2, 3]'
```

### ã‚¹ãƒ†ãƒƒãƒ—3: Skip2-LoRA (6å±¤)ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

```bash
litgpt finetune config_hub/finetune/llama-2-7b/skip2lora.yaml \
  --out_dir out/finetune/skip2lora_6blocks_exp \
  --skip2lora_block_indices '[0, 1, 2, 3, 4, 5]'
```

## 3. çµæœã®æ¯”è¼ƒ

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¯”è¼ƒ

ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯Lightning Studioã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§ç¢ºèªï¼š

```bash
# ãƒ†ãƒ³ã‚½ãƒ«ãƒœãƒ¼ãƒ‰ã§å¯è¦–åŒ–
tensorboard --logdir out/finetune/
```

### å­¦ç¿’é€Ÿåº¦ã®æ¯”è¼ƒ

å„å®Ÿé¨“ã® logs/version_*/metrics.csv ã§ç¢ºèªï¼š

```bash
# ä¾‹: ã‚¹ãƒ†ãƒƒãƒ—ã‚ãŸã‚Šã®æ™‚é–“ã‚’æŠ½å‡º
grep "train_loss" out/finetune/lora_baseline_exp/logs/*/metrics.csv | head -20
```

### ç²¾åº¦ã®æ¯”è¼ƒï¼ˆValidation Lossï¼‰

```python
import pandas as pd
import matplotlib.pyplot as plt

# çµæœã‚’èª­ã¿è¾¼ã‚€
baseline = pd.read_csv("out/finetune/lora_baseline_exp/logs/version_0/metrics.csv")
skip2lora_4 = pd.read_csv("out/finetune/skip2lora_4blocks_exp/logs/version_0/metrics.csv")
skip2lora_6 = pd.read_csv("out/finetune/skip2lora_6blocks_exp/logs/version_0/metrics.csv")

# æå¤±å€¤ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
plt.figure(figsize=(12, 6))
plt.plot(baseline["step"], baseline["val_loss"], label="Traditional LoRA", marker='o')
plt.plot(skip2lora_4["step"], skip2lora_4["val_loss"], label="Skip2-LoRA (4 blocks)", marker='s')
plt.plot(skip2lora_6["step"], skip2lora_6["val_loss"], label="Skip2-LoRA (6 blocks)", marker='^')
plt.xlabel("Training Step")
plt.ylabel("Validation Loss")
plt.legend()
plt.grid(True)
plt.savefig("loss_comparison.png")
plt.show()
```

## 4. è©³ç´°ãªè©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹

### 4.1 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡

```
Traditional LoRA (Query+Value):
  - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½: 2.1M ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
  - å‰Šæ¸›ç‡: 0.025% (å…¨ 7B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸­)

Skip2-LoRA (4 blocks):
  - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½: 0.52M ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
  - å‰Šæ¸›ç‡: 75% (å¾“æ¥LoRAæ¯”)

Skip2-LoRA (6 blocks):
  - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½: 0.79M ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
  - å‰Šæ¸›ç‡: 62% (å¾“æ¥LoRAæ¯”)
```

### 4.2 é€Ÿåº¦æ”¹å–„

```
Backward Pass Time:
  Traditional LoRA:       XXXms
  Skip2-LoRA (4 blocks):  XXXms (XX% é«˜é€Ÿ)
  Skip2-LoRA (6 blocks):  XXXms (XX% é«˜é€Ÿ)

Overall Training Time:
  Traditional LoRA:       XX åˆ†
  Skip2-LoRA (4 blocks):  XX åˆ† (XX% å‰Šæ¸›)
  Skip2-LoRA (6 blocks):  XX åˆ† (XX% å‰Šæ¸›)
```

### 4.3 ãƒ¡ãƒ¢ãƒªå‰Šæ¸›

```
Peak GPU Memory:
  Traditional LoRA:       XX GB
  Skip2-LoRA (4 blocks):  XX GB (XX% å‰Šæ¸›)
  Skip2-LoRA (6 blocks):  XX GB (XX% å‰Šæ¸›)
```

## 5. ç²¾åº¦-åŠ¹ç‡ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®åˆ†æ

### 5.1 æå¤±å€¤ã®åæŸé€Ÿåº¦

å„æ‰‹æ³•ã®ä»¥ä¸‹ã‚’æ¯”è¼ƒï¼š
- **åˆæœŸæå¤±**: ã‚¹ãƒ†ãƒƒãƒ—0ã§ã®æå¤±å€¤
- **æœ€çµ‚æå¤±**: ã‚¹ãƒ†ãƒƒãƒ—100ã§ã®æå¤±å€¤
- **åæŸé€Ÿåº¦**: Î”loss / ã‚¹ãƒ†ãƒƒãƒ—æ•°

### 5.2 ç²¾åº¦ã‚’ä¿è¨¼ã™ã‚‹æœ€å°è¨­å®š

```
é«˜ç²¾åº¦ãŒå¿…è¦ï¼ˆç²¾åº¦é‡è¦–ï¼‰:
  skip2lora_block_indices: [0, 1, 2, 3, 4, 5, 6, 7]
  åŠ¹æœ: æœ€é«˜ç²¾åº¦ã€æœ€å¤§ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ç‡

ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆæ¨å¥¨ï¼‰:
  skip2lora_block_indices: [0, 1, 2, 3, 4, 5]
  åŠ¹æœ: è‰¯å¥½ãªç²¾åº¦ã€60-70% ãƒ¡ãƒ¢ãƒªå‰Šæ¸›

ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ãŒå³ã—ã„ï¼ˆé€Ÿåº¦é‡è¦–ï¼‰:
  skip2lora_block_indices: [0, 1, 2, 3]
  åŠ¹æœ: æœ€é«˜é€Ÿã€ç²¾åº¦ä½ä¸‹ã®å¯èƒ½æ€§
```

## 6. è‡ªå‹•è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§è¤‡æ•°ã®è¨­å®šã‚’ä¸€æ‹¬è©•ä¾¡ï¼š

```python
# evaluate_skip2lora_comparison.py
import subprocess
import json
from pathlib import Path

configs = [
    ("lora_baseline", "lora_baseline.yaml"),
    ("skip2lora_4blocks", "skip2lora.yaml", [0, 1, 2, 3]),
    ("skip2lora_6blocks", "skip2lora.yaml", [0, 1, 2, 3, 4, 5]),
]

results = {}

for name, config, *block_indices in configs:
    cmd = f"litgpt finetune {config} --out_dir out/finetune/{name}_exp"
    if block_indices:
        cmd += f" --skip2lora_block_indices '{block_indices[0]}'"
    
    print(f"Running: {name}")
    subprocess.run(cmd, shell=True)
    
    # çµæœã‚’ä¿å­˜
    results[name] = {
        "config": config,
        "status": "completed",
    }

# çµæœã‚’ä¿å­˜
with open("comparison_results.json", "w") as f:
    json.dump(results, f, indent=2)
```

## 7. æ¨å¥¨ã•ã‚Œã‚‹å®Ÿé¨“ãƒ•ãƒ­ãƒ¼

### æœ€å°å®Ÿé¨“ï¼ˆ5åˆ†ç¨‹åº¦ï¼‰

```bash
# 1. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œ
python benchmark_skip2lora.py
```

### ä¸­è¦æ¨¡å®Ÿé¨“ï¼ˆ1æ™‚é–“ç¨‹åº¦ï¼‰

```bash
# 1. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
python benchmark_skip2lora.py

# 2. å°è¦æ¨¡ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ•°ã‚¹ãƒ†ãƒƒãƒ—ï¼‰ã§ãƒ¡ãƒ¢ãƒªç¢ºèª
litgpt finetune config_hub/finetune/llama-2-7b/lora_baseline.yaml \
  --max_steps 10 --out_dir out/test_baseline

litgpt finetune config_hub/finetune/llama-2-7b/skip2lora.yaml \
  --max_steps 10 --out_dir out/test_skip2lora
```

### è©³ç´°å®Ÿé¨“ï¼ˆæ•°æ™‚é–“ã‹ã‚‰æ•°æ—¥ï¼‰

```bash
# 1. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
python benchmark_skip2lora.py

# 2. è¤‡æ•°ã®è¨­å®šã§ãƒ•ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
for config in lora_baseline skip2lora_4blocks skip2lora_6blocks; do
  litgpt finetune config_hub/finetune/llama-2-7b/${config}.yaml
done

# 3. çµæœåˆ†æ
python analyze_results.py
```

## 8. æ³¨æ„äº‹é …

### 8.1 ç’°å¢ƒè¦ä»¶

- **GPU**: NVIDIA A100 ã¾ãŸã¯åŒç­‰ä»¥ä¸Šæ¨å¥¨
  - 16GB GPU: batch_size=2
  - 24GB GPU: batch_size=4
  - 40GB GPU: batch_size=8

- **ãƒ¡ãƒ¢ãƒª**: 
  - RAM: 64GB ä»¥ä¸Šæ¨å¥¨
  - SSD: 500GB ä»¥ä¸Šï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ç”¨ï¼‰

### 8.2 æ¸¬å®šã®ãƒã‚¤ãƒ³ãƒˆ

1. **è¤‡æ•°å›å®Ÿè¡Œ**: æœ€ä½3å›å®Ÿè¡Œã—ã¦å¹³å‡å€¤ã‚’å–ã‚‹
2. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒªã‚»ãƒƒãƒˆ**: å„å®Ÿé¨“å‰ã« `torch.cuda.empty_cache()` å®Ÿè¡Œ
3. **åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: å…¬å¹³ãªæ¯”è¼ƒã®ãŸã‚åŒã˜ãƒ‡ãƒ¼ã‚¿ã§å®Ÿé¨“

### 8.3 æ—¢çŸ¥ã®æ³¨æ„ç‚¹

- Skip2-LoRA ã¯æ¨è«–æ™‚ã«ã¯é€šå¸¸ã®LoRAäº’æ›ã§ãªã„ï¼ˆãƒãƒ¼ã‚¸ä¸å¯ï¼‰
- è¤‡æ•°å±¤ã®å‡ºåŠ›ã‚’æœ€çµ‚å±¤ã§èåˆã™ã‚‹ãŸã‚ã€è¡¨ç¾åŠ›ãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§
- æœ€é©ãª `skip2lora_block_indices` ã¯ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ä¾å­˜

## 9. çµæœãƒ¬ãƒãƒ¼ãƒˆã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```markdown
# Skip2-LoRA vs Traditional LoRA è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

## å®Ÿé¨“ç’°å¢ƒ
- GPU: XXX
- PyTorch: X.X.X
- LitGPT: main branch
- ãƒ¢ãƒ‡ãƒ«: Llama-2-7B
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: Alpaca-2k

## çµæœã‚µãƒãƒªãƒ¼

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡
| æ‰‹æ³• | ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ | å‰Šæ¸›ç‡ |
|------|-----------------|--------|
| Traditional LoRA | 2.1M | - |
| Skip2-LoRA (4) | 0.5M | 75% |
| Skip2-LoRA (6) | 0.8M | 62% |

### é€Ÿåº¦
| æ‰‹æ³• | Backward (ms) | å‰Šæ¸›ç‡ | å…¨ä½“æ™‚é–“ |
|------|---------------|--------|----------|
| Traditional LoRA | XXX | - | XXXåˆ† |
| Skip2-LoRA (4) | XXX | XX% | XXXåˆ† |
| Skip2-LoRA (6) | XXX | XX% | XXXåˆ† |

### ç²¾åº¦
| æ‰‹æ³• | Final Loss | ç²¾åº¦ä½ä¸‹ |
|------|-----------|---------|
| Traditional LoRA | X.XXX | - |
| Skip2-LoRA (4) | X.XXX | X.X% |
| Skip2-LoRA (6) | X.XXX | X.X% |

## çµè«–

Skip2-LoRA ã¯...
```

## 10. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### Q: ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ãŒè¦‹ã‚‰ã‚Œãªã„
A: ä»¥ä¸‹ã‚’ç¢ºèªï¼š
- `skip2lora_block_indices` ã«ååˆ†ãªå±¤ã‚’æŒ‡å®šã—ã¦ã„ã‚‹ã‹
- Backward pass ã®ãƒ¡ãƒ¢ãƒªã‚’æ¸¬å®šã—ã¦ã„ã‚‹ã‹ï¼ˆForward ã®ã¿ã§ã¯ãªãï¼‰

### Q: ç²¾åº¦ãŒå¤§å¹…ã«ä½ä¸‹ã—ã¦ã„ã‚‹
A: ä»¥ä¸‹ã‚’è©¦ã™ï¼š
- `skip2lora_block_indices` ã«å±¤ã‚’è¿½åŠ 
- `lora_r` ã‚’å¢—ã‚„ã™
- `lora_alpha` ã‚’èª¿æ•´

### Q: é€Ÿåº¦æ”¹å–„ãŒæœŸå¾…ã‚ˆã‚Šå°ã•ã„
A: ä»¥ä¸‹ã‚’ç¢ºèªï¼š
- ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒååˆ†ã«å¤§ãã„ã‹
- GPU ãƒ¡ãƒ¢ãƒªãƒã‚¦ãƒ³ãƒ‰ã«ãªã£ã¦ã„ãªã„ã‹
- ä»–ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒå¹²æ¸‰ã—ã¦ã„ãªã„ã‹

