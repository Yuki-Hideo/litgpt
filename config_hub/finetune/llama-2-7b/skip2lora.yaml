# The path to the base model's checkpoint directory to load for finetuning.
checkpoint_dir: checkpoints/meta-llama/Llama-2-7b-hf

# Directory in which to save checkpoints and logs.
out_dir: out/finetune/skip2lora-llama2-7b

# The precision to use for finetuning. Possible choices: "bf16-true", "bf16-mixed", "32-true".
precision: bf16-true

# If set, quantize the model with this algorithm. See ``tutorials/quantize.md`` for more information.
quantize:

# How many devices/GPUs to use.
devices: 1

# How many nodes to use.
num_nodes: 1

# The LoRA rank.
lora_r: 32

# The LoRA alpha.
lora_alpha: 16

# The LoRA dropout value.
lora_dropout: 0.05

# Skip2-LoRA: Enable skip-connected LoRA for efficient fine-tuning
skip2lora_enabled: true

# Skip2-LoRA: Which block indices to apply Skip2-LoRA to (0-indexed)
# Example: [0, 1, 2, 3, 4, 5] applies Skip2-LoRA to the first 6 layers
skip2lora_block_indices: [0, 1, 2, 3, 4, 5]

# Skip2-LoRA: Where to accumulate and add the Skip2-LoRA outputs
# Options: "lm_head" (default), or specific block index like "31" (for the last block)
skip2lora_output_layer: lm_head

# Disable traditional LoRA on individual components since we're using Skip2-LoRA
lora_query: false
lora_key: false
lora_value: false
lora_projection: false
lora_mlp: false
lora_head: false

# Data-related arguments. If not provided, the default is ``litgpt.data.Alpaca``.
data:
  class_path: litgpt.data.Alpaca2k
  init_args:
    mask_prompt: false
