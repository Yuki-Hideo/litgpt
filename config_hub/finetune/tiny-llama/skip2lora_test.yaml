# Test configuration for Skip2-LoRA with TinyLlama
checkpoint_dir: checkpoints/TinyLlama/TinyLlama-1.1B-Chat-v1.0

# Directory in which to save checkpoints and logs.
out_dir: out/finetune/skip2lora-tinyllama-test

# The precision to use for finetuning.
precision: 32-true

devices: 1
num_nodes: 1

# The LoRA rank.
lora_r: 8

# The LoRA alpha.
lora_alpha: 16

# The LoRA dropout value.
lora_dropout: 0.05

# Skip2-LoRA configuration
# Skip2-LoRA adds learnable skip connections from early layers to the output
skip2lora_enabled: true
skip2lora_block_indices: [0, 1, 2]
skip2lora_output_layer: lm_head

# Enable LoRA on attention and MLP layers
# Skip2-LoRA works in addition to these standard LoRA adapters
lora_query: true
lora_key: false
lora_value: true
lora_projection: false
lora_mlp: true
lora_head: false

# Data configuration
data:
  class_path: litgpt.data.Alpaca2k
  init_args:
    mask_prompt: false

# Training configuration
train:
  save_interval: 100
  log_interval: 1
  global_batch_size: 4
  micro_batch_size: 1
  lr_warmup_steps: 10
  epochs: 1
  max_steps: 50
